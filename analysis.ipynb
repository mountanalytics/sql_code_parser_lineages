{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_grade(strings_list: list, dataframe: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of integers with the complexity grade of the input dataframes transformations\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    substrings_data = {'Transformation': [], 'Occurrences': []}  # Initialize an empty dictionary to store data\n",
    "\n",
    "    for string in strings_list:\n",
    "        total_grade = 0\n",
    "        for index, row in dataframe.iterrows():\n",
    "            substring = row['General_Alias']\n",
    "            grade = row['Complexity_Grade']\n",
    "            occurrences = string.count(substring)\n",
    "            total_grade += occurrences * grade\n",
    "            \n",
    "            # Append to the substrings_data dictionary if occurrences are not 0\n",
    "            if occurrences != 0:\n",
    "                substrings_data['Transformation'].append(substring)\n",
    "                substrings_data['Occurrences'].append(occurrences)\n",
    "        \n",
    "        results.append(total_grade)\n",
    "    \n",
    "    # Create a DataFrame from the substrings_data dictionary\n",
    "    substrings_df = pd.DataFrame(substrings_data)\n",
    "    \n",
    "    return results, substrings_df\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL_NODE</th>\n",
       "      <th>ID</th>\n",
       "      <th>COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>orders</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reviews</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>order_items</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>products</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>payments</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>customers</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loans</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>transactions</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>accounts</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>market_data</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>trades</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dividends</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stocks</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>investors</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>portfolios</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>subscription_reviews</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>subscriptions</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deliveries</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>product_reviews</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>delivery_items</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vendors</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>branches</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>account_types</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>categories</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>subscription_plans</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>d</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              LABEL_NODE  ID  COUNT\n",
       "0                 orders   1      0\n",
       "1                reviews   3      0\n",
       "2            order_items   5      0\n",
       "3               products   6      0\n",
       "4               payments   8      0\n",
       "5              customers  11      0\n",
       "6                  loans  13      0\n",
       "7           transactions  15      0\n",
       "8               accounts  17      0\n",
       "9            market_data  19      0\n",
       "10                trades  21      0\n",
       "11             dividends  24      0\n",
       "12                stocks  26      0\n",
       "13             investors  28      0\n",
       "14            portfolios  29      0\n",
       "15  subscription_reviews  31      0\n",
       "16         subscriptions  35      0\n",
       "17            deliveries  37      0\n",
       "18       product_reviews  39      0\n",
       "19        delivery_items  41      0\n",
       "20               vendors  44      0\n",
       "21              branches  45      0\n",
       "22         account_types  46      0\n",
       "23            categories  47      0\n",
       "24    subscription_plans  48      0\n",
       "25                     d  49      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------- block code to create the files ready for sankey diagram data sources -> calcviews ------------------\n",
    "\n",
    "\n",
    "functions_score = pd.read_excel(\"data/functions_score.xlsx\")[[\"General_Alias\", \"Complexity_Grade\"]]\n",
    "functions_score = functions_score.drop_duplicates()\n",
    "DIR = \"data/output-tables/lineages\"\n",
    "save_DIR = \"report/data\"   \n",
    "list_files = os.listdir(DIR)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    list_files.remove('lineage-merged.csv')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_labels = pd.read_csv(\"data/output-tables/nodes.csv\", sep = ',')\n",
    "#df = pd.read_csv(f\"{DIR}/{list_files[0]}\", sep = ',')\n",
    "df_labels = df_labels.dropna(subset=['FUNCTION'])\n",
    "Sources = pd.DataFrame()\n",
    "\n",
    "# Iterate over rows of the DataFrame\n",
    "for index, row in df_labels.iterrows():\n",
    "    # Check if '@' is not in the 'LABEL_NODE' column of the current row\n",
    "    if 'table' in row['FUNCTION']:\n",
    "        # Extract only the desired columns and rename 'FILTER' to 'COUNT'\n",
    "        filtered_row = row[['LABEL_NODE', 'ID', 'WHERE']].rename({'WHERE': 'COUNT'})\n",
    "        \n",
    "        # Append the filtered row to the empty DataFrame\n",
    "        Sources = pd.concat([Sources, filtered_row.to_frame().transpose()], ignore_index=True)\n",
    "Sources['COUNT'] = 0\n",
    "info_calc = {}\n",
    "\n",
    "Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CALC_VIEW': 'CUSTOMER_BANK_DETAILS',\n",
       "  'SOURCE': ['payments',\n",
       "   'customers',\n",
       "   'loans',\n",
       "   'transactions',\n",
       "   'accounts',\n",
       "   'branches',\n",
       "   'account_types']},\n",
       " {'CALC_VIEW': 'CUSTOMER_ORDER',\n",
       "  'SOURCE': ['orders',\n",
       "   'reviews',\n",
       "   'order_items',\n",
       "   'products',\n",
       "   'payments',\n",
       "   'customers',\n",
       "   'categories']},\n",
       " {'CALC_VIEW': 'CUSTOMER_SUBSCRIPTION_DETAILS',\n",
       "  'SOURCE': ['payments',\n",
       "   'customers',\n",
       "   'subscription_reviews',\n",
       "   'subscriptions',\n",
       "   'subscription_plans']},\n",
       " {'CALC_VIEW': 'INVESTOR_OVERVIEW',\n",
       "  'SOURCE': ['market_data',\n",
       "   'trades',\n",
       "   'dividends',\n",
       "   'stocks',\n",
       "   'investors',\n",
       "   'portfolios']},\n",
       " {'CALC_VIEW': 'VENDOR_PERFORMANCE_ANALYSIS',\n",
       "  'SOURCE': ['products',\n",
       "   'deliveries',\n",
       "   'product_reviews',\n",
       "   'delivery_items',\n",
       "   'vendors',\n",
       "   'categories',\n",
       "   'd']}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this takes into account all the xlsx in the folder which are all the tech lineages from all the calc views\n",
    "for files in list_files:\n",
    "    df = pd.read_csv(f\"{DIR}/{files}\", sep = ',')\n",
    "    source_ids = set(Sources['ID'])\n",
    "    \n",
    "    # Iterate over rows in df['SOURCE_NODE']\n",
    "    for source_id in df['SOURCE_NODE']:\n",
    "        # Check if the source_id exists in source_ids and has not been counted before\n",
    "        if source_id in source_ids:\n",
    "            # Find the index of the ID in Sources\n",
    "            idx = Sources.index[Sources['ID'] == source_id] # !!! ID\n",
    "            # Increment the count for the ID by one\n",
    "            Sources.loc[idx, 'COUNT'] += 1\n",
    "            # Remove the ID from source_ids to ensure it's only counted once\n",
    "            source_ids.remove(source_id)\n",
    "\n",
    "    \n",
    "    # List of unique nodes\n",
    "    nodes = list(set(df['TARGET_NODE']) | set(df['SOURCE_NODE']))\n",
    "    \n",
    "    # Filter label_nodes and function_nodes based on matching IDs\n",
    "    label_nodes = df_labels[df_labels['ID'].isin(nodes)][['ID', 'LABEL_NODE']].rename(columns={'LABEL_NODE': 'LABEL_NODE'})\n",
    "    function_nodes = df_labels[df_labels['ID'].isin(nodes)][['ID', 'FUNCTION']].rename(columns={'FUNCTION': 'FUNCTION'})\n",
    "    \n",
    "    # Count occurrences of each node in TARGET_NODE and SOURCE_NODE columns\n",
    "    target_nodes = df['TARGET_NODE'].value_counts().reset_index().rename(columns={'TARGET_NODE': 'ID', 'count': 'TARGET_COUNT'})\n",
    "    source_nodes = df['SOURCE_NODE'].value_counts().reset_index().rename(columns={'SOURCE_NODE': 'ID', 'count': 'SOURCE_COUNT'})\n",
    "    \n",
    "    # Merge label_nodes and function_nodes on 'ID'\n",
    "    label_function_nodes = pd.merge(label_nodes, function_nodes, on='ID', how='outer')\n",
    "    \n",
    "    # Merge label_function_nodes, target_nodes, and source_nodes on 'ID'\n",
    "    result = pd.merge(label_function_nodes, target_nodes, left_on='ID', right_on='ID', how='outer')\n",
    "    result = pd.merge(result, source_nodes, left_on='ID', right_on='ID', how='outer')\n",
    "    result['TARGET_COUNT'] = result['TARGET_COUNT'].fillna(0)\n",
    "    result['SOURCE_COUNT'] = result['SOURCE_COUNT'].fillna(0)\n",
    "    \n",
    "    info_calc[files.split('-')[1].split('.')[0]] = result\n",
    "    \"\"\" this first part is to calcualtion how much a node is used as a source or a target node based on the columns which are fed into or arise from the node\n",
    "    \"\"\" \n",
    "calc_views = list(info_calc.keys())\n",
    "\n",
    "filtered_data = []\n",
    "for key, df in info_calc.items():\n",
    "    \"\"\" part to get the sources which source feed data into the calc view. This could also be other caluculation views\"\"\"\n",
    "    filtered_df = df[df['FUNCTION'] == 'table']\n",
    "    if not filtered_df.empty:\n",
    "        label_nodes = filtered_df['LABEL_NODE'].tolist()\n",
    "        filtered_data.append({'CALC_VIEW': key, 'SOURCE': label_nodes})\n",
    "\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"this part is to make the csv files which are used for the sankey where sources are coupled to the calc views\"\"\"\n",
    "# Create a new dataframe from the filtered data\n",
    "result_df = pd.DataFrame(filtered_data)\n",
    "result_df = result_df.explode('SOURCE').reset_index(drop=True)\n",
    "Nodes_source = list(np.unique(result_df['CALC_VIEW']))\n",
    "Nodes_source.extend(np.unique(result_df['SOURCE']))\n",
    "Nodes_source = pd.DataFrame(Nodes_source, columns=['Name'])\n",
    "result_df['CALC_ID'],result_df['SOURCE_ID'],result_df['LINK_VALUE'],result_df['COLOR'] = 0,0,1,'aliceblue'\n",
    "\n",
    "for i in range(len(result_df)):\n",
    "    for j in range(len(Nodes_source)):\n",
    "        if result_df.at[i, 'CALC_VIEW'] == Nodes_source.at[j, 'Name']:\n",
    "            result_df.at[i, 'CALC_ID'] = j\n",
    "        elif result_df.at[i, 'SOURCE'] == Nodes_source.at[j, 'Name']:\n",
    "            result_df.at[i, 'SOURCE_ID'] = j\n",
    "\n",
    "            \n",
    "result_df.to_csv(\"data/output-tables/analysis/lineage_calc_source.csv\", index = False)\n",
    "Nodes_source.to_csv(\"data/output-tables/analysis/nodes_calc_source.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                  NaN\n",
      "1                                                  NaN\n",
      "2         COMPARE(loans.end_date, CURRENT_TIMESTAMP())\n",
      "3                                                  NaN\n",
      "4    COMPARE(transactions.transaction_date, DATETIM...\n",
      "5                                                  NaN\n",
      "6    COMPARE(customers.join_date, DATETIME_ADDITION...\n",
      "7                                                  NaN\n",
      "8                                                  NaN\n",
      "9                                                  NaN\n",
      "Name: WHERE, dtype: object\n",
      "0                        EQ(orders.status, 'Completed')\n",
      "1                                                   NaN\n",
      "2                                                   NaN\n",
      "3                                                   NaN\n",
      "4                                                   NaN\n",
      "5                                                   NaN\n",
      "6                                                   NaN\n",
      "7                                                   NaN\n",
      "8                                                   NaN\n",
      "9     AND(COMPARE(orders.order_date, DATETIME_ADDITI...\n",
      "10    COMPARE(customers.signup_date, DATETIME_ADDITI...\n",
      "11                                                  NaN\n",
      "12                                                  NaN\n",
      "Name: WHERE, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'payments' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'json_data1@subquery1_2' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'json_data0@subquery1_5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'orders' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                  NaN\n",
      "1                                                  NaN\n",
      "2                                                  NaN\n",
      "3                                                  NaN\n",
      "4    COMPARE(payments.payment_date, DATETIME_ADDITI...\n",
      "5                                                  NaN\n",
      "6    COMPARE(customers.signup_date, DATETIME_ADDITI...\n",
      "7                                                  NaN\n",
      "8                                                  NaN\n",
      "Name: WHERE, dtype: object\n",
      "0                                                   NaN\n",
      "1                                                   NaN\n",
      "2                                                   NaN\n",
      "3                                                   NaN\n",
      "4     COMPARE(market_data.market_date, DATETIME_ADDI...\n",
      "5     COMPARE(dividends.dividend_date, DATETIME_ADDI...\n",
      "6                                                   NaN\n",
      "7     COMPARE(trades.trade_date, DATETIME_ADDITION('...\n",
      "8                                                   NaN\n",
      "9     COMPARE(investors.join_date, DATETIME_ADDITION...\n",
      "10                                                  NaN\n",
      "11                                                  NaN\n",
      "Name: WHERE, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'json_data3@subquery1_3' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'subscription_reviews' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'json_data2@subquery2_1' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'market_data' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                   NaN\n",
      "1                                                   NaN\n",
      "2                                                   NaN\n",
      "3                                                   NaN\n",
      "4                                                   NaN\n",
      "5                                                   NaN\n",
      "6                                                   NaN\n",
      "7     COMPARE(deliveries.delivery_date, DATETIME_ADD...\n",
      "8     COMPARE(vendors.contract_start_date, DATETIME_...\n",
      "9                                                   NaN\n",
      "10                                                  NaN\n",
      "11                                                  NaN\n",
      "Name: WHERE, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'json_data4@subquery1_4' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'deliveries' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" these table names correspond with the table names in the report creation file\"\"\"\n",
    "trans_data = pd.DataFrame(columns=['Transformation', 'Occurrences'])\n",
    "\n",
    "table11 = pd.DataFrame(columns=['Calculation view','Number of nodes', 'Number of transformations', 'Number of filters'])\n",
    "\n",
    "# --------------- block code to create technical lineages for a calculation view ------------------\n",
    "\n",
    "table212 = pd.DataFrame(columns=['Calculation view','Node', 'Transformation', 'Complexity Score'])\n",
    "table2122 = pd.DataFrame(columns=['Calculation view', 'Transformation count', 'Summation complexity Score'])\n",
    "table211 = pd.DataFrame(columns=['Calculation view','Node', 'Transformation', 'Complexity Score'])\n",
    "\n",
    "for files in list_files:\n",
    "    df = pd.read_csv(f\"{DIR}/{files}\", sep = ',')\n",
    "    #df = pd.read_csv(f\"{DIR}/{list_files[8]}\", sep = ',')\n",
    "    nodes = list(set(df['TARGET_NODE']) | set(df['SOURCE_NODE']))\n",
    "    \n",
    "    # Filter label_nodes and function_nodes based on matching IDs\n",
    "    label_nodes = df_labels[df_labels['ID'].isin(nodes)][['ID', 'LABEL_NODE', 'FUNCTION', 'ON','WHERE']].rename(columns={'LABEL_NODE': 'LABEL_NODE', 'FUNCTION': 'FUNCTION', 'ON': 'ON', 'WHERE': 'WHERE'})\n",
    "    label_nodes = label_nodes.reset_index(drop=True)\n",
    "    \n",
    "    Data = df[['SOURCE_NODE','SOURCE_FIELD','TARGET_NODE','TARGET_FIELD','TRANSFORMATION']].copy()\n",
    "    sub_join = list(label_nodes['ON'])\n",
    "    \n",
    "    sub_join = [ast.literal_eval(item) if isinstance(item, str) else item for item in sub_join]\n",
    "    # Iterate through the list and add 'LABEL_NODE' to dictionaries\n",
    "    for i in range(len(sub_join)):\n",
    "        if isinstance(sub_join[i], dict):\n",
    "            print(i)\n",
    "            # Extract the corresponding 'LABEL_NODE' from label_nodes DataFrame based on its index\n",
    "            label_node = label_nodes.iloc[i]['LABEL_NODE']\n",
    "            sub_join[i]['LABEL_NODE'] = label_node\n",
    "    sub_join = [item for item in sub_join if isinstance(item, dict)]\n",
    "    list_filters = []\n",
    "\n",
    "    print(label_nodes['WHERE'])\n",
    "    for filter_value, label_node in zip(label_nodes['WHERE'], label_nodes['LABEL_NODE']):\n",
    "        if isinstance(filter_value, str):\n",
    "            list_filters.append({'filter': filter_value, 'LABEL_NODE': label_node, 'Field' : filter_value})#.split('\"')[1]})\n",
    "    \n",
    "    \n",
    "    Data[\"ON\"] = np.nan\n",
    "    Data[\"WHERE\"] = np.nan\n",
    "    \n",
    "    for i in range(len(Data)):\n",
    "        for j in range(len(label_nodes)):\n",
    "            if Data.at[i, 'SOURCE_NODE'] == label_nodes.at[j, 'ID']:\n",
    "                Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
    "            if Data.at[i, 'TARGET_NODE'] == label_nodes.at[j, 'ID']:\n",
    "                Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
    "        for k in range(len(sub_join)):\n",
    "            if 'LABEL_NODE' in sub_join[k] and sub_join[k]['LABEL_NODE'] == Data.at[i, 'TARGET_NODE'] and sub_join[k].get('JoinVariable') == Data.at[i, 'TARGET_FIELD']:\n",
    "                updated_dict = sub_join[k].copy()  # Make a copy of the dictionary\n",
    "                updated_dict.pop('LABEL_NODE', None)  # Remove 'LABEL_NODE' key\n",
    "                Data.at[i, 'ON'] = str(updated_dict)\n",
    "        for l in range(len(list_filters)):\n",
    "            if list_filters[l]['LABEL_NODE'] == Data.at[i, 'TARGET_NODE'] and list_filters[l].get('Field') == Data.at[i, 'TARGET_FIELD']: \n",
    "                Data.at[i, 'WHERE'] = list_filters[l].get('filter')\n",
    "    \n",
    "    strings_list = list(Data[\"TRANSFORMATION\"])\n",
    "    strings_list = [str(x) for x in strings_list]\n",
    "    grades_list, substrings_df = calculate_grade(strings_list, functions_score)\n",
    "    substrings_df = substrings_df.groupby('Transformation', as_index=False).sum().reset_index(drop=True)\n",
    "    Data[\"Complexity_Score\"] = grades_list\n",
    "    #Data = Data.dropna(subset=['JOIN_ARGU', 'FILTER', 'TRANSFORMATION'], how='all')\n",
    "    trans_data = pd.concat([trans_data, substrings_df])\n",
    "\n",
    "\n",
    "    #------------------- block for aggregation ---------------------------\n",
    "    \n",
    "    unique_trans = Data['TRANSFORMATION'].dropna().nunique()\n",
    "    unique_filter = Data['WHERE'].dropna().nunique()\n",
    "    function_counts = label_nodes[\"FUNCTION\"].value_counts()\n",
    "    Data_final_tech = Data.dropna(subset=['ON', 'WHERE', 'TRANSFORMATION'], how='all')\n",
    "    \n",
    "    \n",
    "    Data_final = Data.dropna(subset=['TRANSFORMATION'], how='all')\n",
    "    \n",
    "    # Drop duplicate rows based on selected columns\n",
    "    filtered_df = Data_final.drop_duplicates(subset=['SOURCE_NODE', 'TRANSFORMATION']).reset_index(drop=True)\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        if str(row[\"SOURCE_FIELD\"]) in str(row[\"SOURCE_NODE\"]):\n",
    "            filtered_df.drop(index=index, inplace=True)\n",
    "    filtered_df = filtered_df.reset_index(drop=True)   \n",
    "    filtered_df = filtered_df[['SOURCE_NODE', 'TRANSFORMATION', 'Complexity_Score']]\n",
    "    filtered_df = filtered_df.rename(columns={'SOURCE_NODE': \"Node\", 'TRANSFORMATION' : 'Transformation', 'Complexity_Score' : 'Complexity Score'})\n",
    "    filtered_df['Calculation view'] = files.split('-')[1].split('.')[0]\n",
    "    filtered_df = filtered_df.reindex(columns=['Calculation view', 'Node', 'Transformation', 'Complexity Score'])\n",
    "    \n",
    "    table212 = pd.concat([table212, filtered_df])\n",
    "    filtered_df = filtered_df.sort_values(by='Complexity Score', ascending=False).head(1)\n",
    "    table211 = pd.concat([table211, filtered_df])\n",
    "    \n",
    "    temp = {'Calculation view': files.split('-')[1].split('.')[0],'Number of nodes' : sum(function_counts), 'Number of transformations' : unique_trans, 'Number of filters' : unique_filter}\n",
    "    table11.loc[len(table11)] = temp\n",
    "    calc_scores =  Data[Data['TRANSFORMATION'].notna()].drop_duplicates(subset='TRANSFORMATION').reset_index(drop=True)\n",
    "    temp = {'Calculation view' : files.split('-')[1].split('.')[0], 'Transformation count' : unique_trans, 'Summation complexity Score' : sum(calc_scores[\"Complexity_Score\"])}\n",
    "    table2122.loc[len(table2122)] = temp\n",
    "    if files == 'lineage-Q_AccountsPayable.csv':\n",
    "        substrings_df.to_csv(f\"{save_DIR}/substrings_df.csv\",index = False)\n",
    "        Data_final_tech.to_csv(f\"{save_DIR}/Data_final_tech .csv\",index = False)\n",
    "        Account_payable_tech_lineage = Data\n",
    "table212 = table212.sort_values(by='Complexity Score', ascending=False).head(5).reset_index(drop=True)  \n",
    "table2122 = table2122.sort_values(by='Summation complexity Score', ascending=False).head(5).reset_index(drop=True)  \n",
    "\n",
    "trans_data = trans_data.groupby('Transformation', as_index=False).sum().reset_index(drop=True)   \n",
    "table112 = Sources.sort_values(by='COUNT',ascending=False) \n",
    "table112 = table112.drop(columns=['ID'], axis=1).head(5).reset_index(drop=True)\n",
    "#print(sorted_sources)\n",
    "calc_names = []\n",
    "table22 = pd.DataFrame(columns=['Calculation view','Input calculation view'])\n",
    "table31 = pd.DataFrame(columns=['Calculation view','Data source','Columns used','Columns in source','Percentage columns used'])\n",
    "columns_tables = pd.read_excel(\"data/Columns_sources.xlsx\").dropna().reset_index(drop=True)\n",
    "for files in list_files:\n",
    "    calc_names.append(files.split('-')[1].split('.')[0])\n",
    "for i in info_calc.keys():\n",
    "    for j in range(len(info_calc[i])):\n",
    "        if info_calc[i]['LABEL_NODE'][j] in calc_names:\n",
    "            temp = {'Calculation view': i,'Input calculation view' : info_calc[i]['LABEL_NODE'][j]}\n",
    "            table22.loc[len(table22)] = temp\n",
    "        for k in range(len(columns_tables)):  \n",
    "            if info_calc[i]['LABEL_NODE'][j] == columns_tables['LABEL_NODE'][k] and info_calc[i]['FUNCTION'][j] == \"DataSources\":\n",
    "                temp = {'Calculation view': i,'Data source' : info_calc[i]['LABEL_NODE'][j], 'Columns used' : info_calc[i]['SOURCE_COUNT'][j], 'Columns in source' :  columns_tables['COUNT'][k], \"Percentage columns used\" : info_calc[i]['SOURCE_COUNT'][j]/columns_tables['COUNT'][k]}\n",
    "                table31.loc[len(table31)] = temp\n",
    "    \n",
    "table113 = trans_data.sort_values(by='Occurrences', ascending=False).head(5).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "table11.to_csv(f\"{save_DIR}/table11.csv\",index = False)\n",
    "table112.to_csv(f\"{save_DIR}/table112.csv\",index = False)\n",
    "table113.to_csv(f\"{save_DIR}/table113.csv\",index = False)\n",
    "table2122.to_csv(f\"{save_DIR}/table2122.csv\",index = False)\n",
    "table211.to_csv(f\"{save_DIR}/table211.csv\",index = False)\n",
    "table212.to_csv(f\"{save_DIR}/table212.csv\",index = False)\n",
    "table22.to_csv(f\"{save_DIR}/table22.csv\",index = False)\n",
    "table31.to_csv(f\"{save_DIR}/table31.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rabo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
