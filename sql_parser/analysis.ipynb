{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_grade(strings_list: list, dataframe: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of integers with the complexity grade of the input dataframes transformations\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    substrings_data = {'Transformation': [], 'Occurrences': []}  # Initialize an empty dictionary to store data\n",
    "\n",
    "    for string in strings_list:\n",
    "        total_grade = 0\n",
    "        for index, row in dataframe.iterrows():\n",
    "            substring = row['General_Alias']\n",
    "            grade = row['Complexity_Grade']\n",
    "            occurrences = string.count(substring)\n",
    "            total_grade += occurrences * grade\n",
    "            \n",
    "            # Append to the substrings_data dictionary if occurrences are not 0\n",
    "            if occurrences != 0:\n",
    "                substrings_data['Transformation'].append(substring)\n",
    "                substrings_data['Occurrences'].append(occurrences)\n",
    "        \n",
    "        results.append(total_grade)\n",
    "    \n",
    "    # Create a DataFrame from the substrings_data dictionary\n",
    "    substrings_df = pd.DataFrame(substrings_data)\n",
    "    \n",
    "    return results, substrings_df\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL_NODE</th>\n",
       "      <th>ID</th>\n",
       "      <th>COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>orders</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reviews</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>order_items</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>products</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>payments</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>customers</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loans</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>transactions</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>accounts</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>market_data</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>trades</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dividends</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stocks</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>investors</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>portfolios</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>subscription_reviews</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>subscriptions</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deliveries</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>product_reviews</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>delivery_items</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vendors</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>branches</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>account_types</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>categories</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>subscription_plans</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>d</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              LABEL_NODE  ID  COUNT\n",
       "0                 orders   1      0\n",
       "1                reviews   3      0\n",
       "2            order_items   5      0\n",
       "3               products   6      0\n",
       "4               payments   8      0\n",
       "5              customers  11      0\n",
       "6                  loans  13      0\n",
       "7           transactions  15      0\n",
       "8               accounts  17      0\n",
       "9            market_data  19      0\n",
       "10                trades  21      0\n",
       "11             dividends  24      0\n",
       "12                stocks  26      0\n",
       "13             investors  28      0\n",
       "14            portfolios  29      0\n",
       "15  subscription_reviews  31      0\n",
       "16         subscriptions  35      0\n",
       "17            deliveries  37      0\n",
       "18       product_reviews  39      0\n",
       "19        delivery_items  41      0\n",
       "20               vendors  44      0\n",
       "21              branches  45      0\n",
       "22         account_types  46      0\n",
       "23            categories  47      0\n",
       "24    subscription_plans  48      0\n",
       "25                     d  49      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------- block code to create the files ready for sankey diagram data sources -> calcviews ------------------\n",
    "\n",
    "\n",
    "functions_score = pd.read_excel(\"data/functions_score.xlsx\")[[\"General_Alias\", \"Complexity_Grade\"]]\n",
    "functions_score = functions_score.drop_duplicates()\n",
    "DIR = \"data/output-tables/lineages\"\n",
    "save_DIR = \"report/data\"   \n",
    "list_files = os.listdir(DIR)\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    list_files.remove('lineage-merged.csv')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_labels = pd.read_csv(\"data/output-tables/nodes.csv\", sep = ',')\n",
    "#df = pd.read_csv(f\"{DIR}/{list_files[0]}\", sep = ',')\n",
    "df_labels = df_labels.dropna(subset=['FUNCTION'])\n",
    "Sources = pd.DataFrame()\n",
    "\n",
    "# Iterate over rows of the DataFrame\n",
    "for index, row in df_labels.iterrows():\n",
    "    # Check if '@' is not in the 'LABEL_NODE' column of the current row\n",
    "    if 'table' in row['FUNCTION']:\n",
    "        # Extract only the desired columns and rename 'FILTER' to 'COUNT'\n",
    "        filtered_row = row[['LABEL_NODE', 'ID', 'WHERE']].rename({'WHERE': 'COUNT'})\n",
    "        \n",
    "        # Append the filtered row to the empty DataFrame\n",
    "        Sources = pd.concat([Sources, filtered_row.to_frame().transpose()], ignore_index=True)\n",
    "Sources['COUNT'] = 0\n",
    "info_calc = {}\n",
    "\n",
    "Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CALC_VIEW': 'CUSTOMER_BANK_DETAILS',\n",
       "  'SOURCE': ['payments',\n",
       "   'customers',\n",
       "   'loans',\n",
       "   'transactions',\n",
       "   'accounts',\n",
       "   'branches',\n",
       "   'account_types']},\n",
       " {'CALC_VIEW': 'CUSTOMER_ORDER',\n",
       "  'SOURCE': ['orders',\n",
       "   'reviews',\n",
       "   'order_items',\n",
       "   'products',\n",
       "   'payments',\n",
       "   'customers',\n",
       "   'categories']},\n",
       " {'CALC_VIEW': 'CUSTOMER_SUBSCRIPTION_DETAILS',\n",
       "  'SOURCE': ['payments',\n",
       "   'customers',\n",
       "   'subscription_reviews',\n",
       "   'subscriptions',\n",
       "   'subscription_plans']},\n",
       " {'CALC_VIEW': 'INVESTOR_OVERVIEW',\n",
       "  'SOURCE': ['market_data',\n",
       "   'trades',\n",
       "   'dividends',\n",
       "   'stocks',\n",
       "   'investors',\n",
       "   'portfolios']},\n",
       " {'CALC_VIEW': 'VENDOR_PERFORMANCE_ANALYSIS',\n",
       "  'SOURCE': ['products',\n",
       "   'deliveries',\n",
       "   'product_reviews',\n",
       "   'delivery_items',\n",
       "   'vendors',\n",
       "   'categories',\n",
       "   'd']}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this takes into account all the xlsx in the folder which are all the tech lineages from all the calc views\n",
    "for files in list_files:\n",
    "    df = pd.read_csv(f\"{DIR}/{files}\", sep = ',')\n",
    "    source_ids = set(Sources['ID'])\n",
    "    \n",
    "    # Iterate over rows in df['SOURCE_NODE']\n",
    "    for source_id in df['SOURCE_NODE']:\n",
    "        # Check if the source_id exists in source_ids and has not been counted before\n",
    "        if source_id in source_ids:\n",
    "            # Find the index of the ID in Sources\n",
    "            idx = Sources.index[Sources['ID'] == source_id] # !!! ID\n",
    "            # Increment the count for the ID by one\n",
    "            Sources.loc[idx, 'COUNT'] += 1\n",
    "            # Remove the ID from source_ids to ensure it's only counted once\n",
    "            source_ids.remove(source_id)\n",
    "\n",
    "    \n",
    "    # List of unique nodes\n",
    "    nodes = list(set(df['TARGET_NODE']) | set(df['SOURCE_NODE']))\n",
    "    \n",
    "    # Filter label_nodes and function_nodes based on matching IDs\n",
    "    label_nodes = df_labels[df_labels['ID'].isin(nodes)][['ID', 'LABEL_NODE']].rename(columns={'LABEL_NODE': 'LABEL_NODE'})\n",
    "    function_nodes = df_labels[df_labels['ID'].isin(nodes)][['ID', 'FUNCTION']].rename(columns={'FUNCTION': 'FUNCTION'})\n",
    "    \n",
    "    # Count occurrences of each node in TARGET_NODE and SOURCE_NODE columns\n",
    "    target_nodes = df['TARGET_NODE'].value_counts().reset_index().rename(columns={'TARGET_NODE': 'ID', 'count': 'TARGET_COUNT'})\n",
    "    source_nodes = df['SOURCE_NODE'].value_counts().reset_index().rename(columns={'SOURCE_NODE': 'ID', 'count': 'SOURCE_COUNT'})\n",
    "    \n",
    "    # Merge label_nodes and function_nodes on 'ID'\n",
    "    label_function_nodes = pd.merge(label_nodes, function_nodes, on='ID', how='outer')\n",
    "    \n",
    "    # Merge label_function_nodes, target_nodes, and source_nodes on 'ID'\n",
    "    result = pd.merge(label_function_nodes, target_nodes, left_on='ID', right_on='ID', how='outer')\n",
    "    result = pd.merge(result, source_nodes, left_on='ID', right_on='ID', how='outer')\n",
    "    result['TARGET_COUNT'] = result['TARGET_COUNT'].fillna(0)\n",
    "    result['SOURCE_COUNT'] = result['SOURCE_COUNT'].fillna(0)\n",
    "    \n",
    "    info_calc[files.split('-')[1].split('.')[0]] = result\n",
    "    \"\"\" this first part is to calcualtion how much a node is used as a source or a target node based on the columns which are fed into or arise from the node\n",
    "    \"\"\" \n",
    "calc_views = list(info_calc.keys())\n",
    "\n",
    "filtered_data = []\n",
    "for key, df in info_calc.items():\n",
    "    \"\"\" part to get the sources which source feed data into the calc view. This could also be other caluculation views\"\"\"\n",
    "    filtered_df = df[df['FUNCTION'] == 'table']\n",
    "    if not filtered_df.empty:\n",
    "        label_nodes = filtered_df['LABEL_NODE'].tolist()\n",
    "        filtered_data.append({'CALC_VIEW': key, 'SOURCE': label_nodes})\n",
    "\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"this part is to make the csv files which are used for the sankey where sources are coupled to the calc views\"\"\"\n",
    "# Create a new dataframe from the filtered data\n",
    "result_df = pd.DataFrame(filtered_data)\n",
    "result_df = result_df.explode('SOURCE').reset_index(drop=True)\n",
    "Nodes_source = list(np.unique(result_df['CALC_VIEW']))\n",
    "Nodes_source.extend(np.unique(result_df['SOURCE']))\n",
    "Nodes_source = pd.DataFrame(Nodes_source, columns=['Name'])\n",
    "result_df['CALC_ID'],result_df['SOURCE_ID'],result_df['LINK_VALUE'],result_df['COLOR'] = 0,0,1,'aliceblue'\n",
    "\n",
    "for i in range(len(result_df)):\n",
    "    for j in range(len(Nodes_source)):\n",
    "        if result_df.at[i, 'CALC_VIEW'] == Nodes_source.at[j, 'Name']:\n",
    "            result_df.at[i, 'CALC_ID'] = j\n",
    "        elif result_df.at[i, 'SOURCE'] == Nodes_source.at[j, 'Name']:\n",
    "            result_df.at[i, 'SOURCE_ID'] = j\n",
    "\n",
    "            \n",
    "result_df.to_csv(\"data/output-tables/analysis/lineage_calc_source.csv\", index = False)\n",
    "Nodes_source.to_csv(\"data/output-tables/analysis/nodes_calc_source.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                  NaN\n",
      "1                                                  NaN\n",
      "2         COMPARE(loans.end_date, CURRENT_TIMESTAMP())\n",
      "3                                                  NaN\n",
      "4    COMPARE(transactions.transaction_date, DATETIM...\n",
      "5                                                  NaN\n",
      "6    COMPARE(customers.join_date, DATETIME_ADDITION...\n",
      "7                                                  NaN\n",
      "8                                                  NaN\n",
      "9                                                  NaN\n",
      "Name: WHERE, dtype: object\n",
      "0                        EQ(orders.status, 'Completed')\n",
      "1                                                   NaN\n",
      "2                                                   NaN\n",
      "3                                                   NaN\n",
      "4                                                   NaN\n",
      "5                                                   NaN\n",
      "6                                                   NaN\n",
      "7                                                   NaN\n",
      "8                                                   NaN\n",
      "9     AND(COMPARE(orders.order_date, DATETIME_ADDITI...\n",
      "10    COMPARE(customers.signup_date, DATETIME_ADDITI...\n",
      "11                                                  NaN\n",
      "12                                                  NaN\n",
      "Name: WHERE, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'payments' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'json_data1@subquery1_2' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'json_data0@subquery1_5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'orders' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                  NaN\n",
      "1                                                  NaN\n",
      "2                                                  NaN\n",
      "3                                                  NaN\n",
      "4    COMPARE(payments.payment_date, DATETIME_ADDITI...\n",
      "5                                                  NaN\n",
      "6    COMPARE(customers.signup_date, DATETIME_ADDITI...\n",
      "7                                                  NaN\n",
      "8                                                  NaN\n",
      "Name: WHERE, dtype: object\n",
      "0                                                   NaN\n",
      "1                                                   NaN\n",
      "2                                                   NaN\n",
      "3                                                   NaN\n",
      "4     COMPARE(market_data.market_date, DATETIME_ADDI...\n",
      "5     COMPARE(dividends.dividend_date, DATETIME_ADDI...\n",
      "6                                                   NaN\n",
      "7     COMPARE(trades.trade_date, DATETIME_ADDITION('...\n",
      "8                                                   NaN\n",
      "9     COMPARE(investors.join_date, DATETIME_ADDITION...\n",
      "10                                                  NaN\n",
      "11                                                  NaN\n",
      "Name: WHERE, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'json_data3@subquery1_3' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'subscription_reviews' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'json_data2@subquery2_1' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'market_data' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                   NaN\n",
      "1                                                   NaN\n",
      "2                                                   NaN\n",
      "3                                                   NaN\n",
      "4                                                   NaN\n",
      "5                                                   NaN\n",
      "6                                                   NaN\n",
      "7     COMPARE(deliveries.delivery_date, DATETIME_ADD...\n",
      "8     COMPARE(vendors.contract_start_date, DATETIME_...\n",
      "9                                                   NaN\n",
      "10                                                  NaN\n",
      "11                                                  NaN\n",
      "Name: WHERE, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:49: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'json_data4@subquery1_4' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
      "C:\\Users\\PietroGarroni\\AppData\\Local\\Temp\\ipykernel_23788\\1416913803.py:47: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'deliveries' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" these table names correspond with the table names in the report creation file\"\"\"\n",
    "trans_data = pd.DataFrame(columns=['Transformation', 'Occurrences'])\n",
    "\n",
    "table11 = pd.DataFrame(columns=['Calculation view','Number of nodes', 'Number of transformations', 'Number of filters'])\n",
    "\n",
    "# --------------- block code to create technical lineages for a calculation view ------------------\n",
    "\n",
    "table212 = pd.DataFrame(columns=['Calculation view','Node', 'Transformation', 'Complexity Score'])\n",
    "table2122 = pd.DataFrame(columns=['Calculation view', 'Transformation count', 'Summation complexity Score'])\n",
    "table211 = pd.DataFrame(columns=['Calculation view','Node', 'Transformation', 'Complexity Score'])\n",
    "\n",
    "for files in list_files:\n",
    "    df = pd.read_csv(f\"{DIR}/{files}\", sep = ',')\n",
    "    #df = pd.read_csv(f\"{DIR}/{list_files[8]}\", sep = ',')\n",
    "    nodes = list(set(df['TARGET_NODE']) | set(df['SOURCE_NODE']))\n",
    "    \n",
    "    # Filter label_nodes and function_nodes based on matching IDs\n",
    "    label_nodes = df_labels[df_labels['ID'].isin(nodes)][['ID', 'LABEL_NODE', 'FUNCTION', 'ON','WHERE']].rename(columns={'LABEL_NODE': 'LABEL_NODE', 'FUNCTION': 'FUNCTION', 'ON': 'ON', 'WHERE': 'WHERE'})\n",
    "    label_nodes = label_nodes.reset_index(drop=True)\n",
    "    \n",
    "    Data = df[['SOURCE_NODE','SOURCE_FIELD','TARGET_NODE','TARGET_FIELD','TRANSFORMATION']].copy()\n",
    "    sub_join = list(label_nodes['ON'])\n",
    "    \n",
    "    sub_join = [ast.literal_eval(item) if isinstance(item, str) else item for item in sub_join]\n",
    "    # Iterate through the list and add 'LABEL_NODE' to dictionaries\n",
    "    for i in range(len(sub_join)):\n",
    "        if isinstance(sub_join[i], dict):\n",
    "            print(i)\n",
    "            # Extract the corresponding 'LABEL_NODE' from label_nodes DataFrame based on its index\n",
    "            label_node = label_nodes.iloc[i]['LABEL_NODE']\n",
    "            sub_join[i]['LABEL_NODE'] = label_node\n",
    "    sub_join = [item for item in sub_join if isinstance(item, dict)]\n",
    "    list_filters = []\n",
    "\n",
    "    print(label_nodes['WHERE'])\n",
    "    for filter_value, label_node in zip(label_nodes['WHERE'], label_nodes['LABEL_NODE']):\n",
    "        if isinstance(filter_value, str):\n",
    "            list_filters.append({'filter': filter_value, 'LABEL_NODE': label_node, 'Field' : filter_value})#.split('\"')[1]})\n",
    "    \n",
    "    \n",
    "    Data[\"ON\"] = np.nan\n",
    "    Data[\"WHERE\"] = np.nan\n",
    "    \n",
    "    for i in range(len(Data)):\n",
    "        for j in range(len(label_nodes)):\n",
    "            if Data.at[i, 'SOURCE_NODE'] == label_nodes.at[j, 'ID']:\n",
    "                Data.at[i, 'SOURCE_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
    "            if Data.at[i, 'TARGET_NODE'] == label_nodes.at[j, 'ID']:\n",
    "                Data.at[i, 'TARGET_NODE'] = label_nodes.at[j, 'LABEL_NODE']\n",
    "        for k in range(len(sub_join)):\n",
    "            if 'LABEL_NODE' in sub_join[k] and sub_join[k]['LABEL_NODE'] == Data.at[i, 'TARGET_NODE'] and sub_join[k].get('JoinVariable') == Data.at[i, 'TARGET_FIELD']:\n",
    "                updated_dict = sub_join[k].copy()  # Make a copy of the dictionary\n",
    "                updated_dict.pop('LABEL_NODE', None)  # Remove 'LABEL_NODE' key\n",
    "                Data.at[i, 'ON'] = str(updated_dict)\n",
    "        for l in range(len(list_filters)):\n",
    "            if list_filters[l]['LABEL_NODE'] == Data.at[i, 'TARGET_NODE'] and list_filters[l].get('Field') == Data.at[i, 'TARGET_FIELD']: \n",
    "                Data.at[i, 'WHERE'] = list_filters[l].get('filter')\n",
    "    \n",
    "    strings_list = list(Data[\"TRANSFORMATION\"])\n",
    "    strings_list = [str(x) for x in strings_list]\n",
    "    grades_list, substrings_df = calculate_grade(strings_list, functions_score)\n",
    "    substrings_df = substrings_df.groupby('Transformation', as_index=False).sum().reset_index(drop=True)\n",
    "    Data[\"Complexity_Score\"] = grades_list\n",
    "    #Data = Data.dropna(subset=['JOIN_ARGU', 'FILTER', 'TRANSFORMATION'], how='all')\n",
    "    trans_data = pd.concat([trans_data, substrings_df])\n",
    "\n",
    "\n",
    "    #------------------- block for aggregation ---------------------------\n",
    "    \n",
    "    unique_trans = Data['TRANSFORMATION'].dropna().nunique()\n",
    "    unique_filter = Data['WHERE'].dropna().nunique()\n",
    "    function_counts = label_nodes[\"FUNCTION\"].value_counts()\n",
    "    Data_final_tech = Data.dropna(subset=['ON', 'WHERE', 'TRANSFORMATION'], how='all')\n",
    "    \n",
    "    \n",
    "    Data_final = Data.dropna(subset=['TRANSFORMATION'], how='all')\n",
    "    \n",
    "    # Drop duplicate rows based on selected columns\n",
    "    filtered_df = Data_final.drop_duplicates(subset=['SOURCE_NODE', 'TRANSFORMATION']).reset_index(drop=True)\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        if str(row[\"SOURCE_FIELD\"]) in str(row[\"SOURCE_NODE\"]):\n",
    "            filtered_df.drop(index=index, inplace=True)\n",
    "    filtered_df = filtered_df.reset_index(drop=True)   \n",
    "    filtered_df = filtered_df[['SOURCE_NODE', 'TRANSFORMATION', 'Complexity_Score']]\n",
    "    filtered_df = filtered_df.rename(columns={'SOURCE_NODE': \"Node\", 'TRANSFORMATION' : 'Transformation', 'Complexity_Score' : 'Complexity Score'})\n",
    "    filtered_df['Calculation view'] = files.split('-')[1].split('.')[0]\n",
    "    filtered_df = filtered_df.reindex(columns=['Calculation view', 'Node', 'Transformation', 'Complexity Score'])\n",
    "    \n",
    "    table212 = pd.concat([table212, filtered_df])\n",
    "    filtered_df = filtered_df.sort_values(by='Complexity Score', ascending=False).head(1)\n",
    "    table211 = pd.concat([table211, filtered_df])\n",
    "    \n",
    "    temp = {'Calculation view': files.split('-')[1].split('.')[0],'Number of nodes' : sum(function_counts), 'Number of transformations' : unique_trans, 'Number of filters' : unique_filter}\n",
    "    table11.loc[len(table11)] = temp\n",
    "    calc_scores =  Data[Data['TRANSFORMATION'].notna()].drop_duplicates(subset='TRANSFORMATION').reset_index(drop=True)\n",
    "    temp = {'Calculation view' : files.split('-')[1].split('.')[0], 'Transformation count' : unique_trans, 'Summation complexity Score' : sum(calc_scores[\"Complexity_Score\"])}\n",
    "    table2122.loc[len(table2122)] = temp\n",
    "    if files == 'lineage-Q_AccountsPayable.csv':\n",
    "        substrings_df.to_csv(f\"{save_DIR}/substrings_df.csv\",index = False)\n",
    "        Data_final_tech.to_csv(f\"{save_DIR}/Data_final_tech .csv\",index = False)\n",
    "        Account_payable_tech_lineage = Data\n",
    "table212 = table212.sort_values(by='Complexity Score', ascending=False).head(5).reset_index(drop=True)  \n",
    "table2122 = table2122.sort_values(by='Summation complexity Score', ascending=False).head(5).reset_index(drop=True)  \n",
    "\n",
    "trans_data = trans_data.groupby('Transformation', as_index=False).sum().reset_index(drop=True)   \n",
    "table112 = Sources.sort_values(by='COUNT',ascending=False) \n",
    "table112 = table112.drop(columns=['ID'], axis=1).head(5).reset_index(drop=True)\n",
    "#print(sorted_sources)\n",
    "calc_names = []\n",
    "table22 = pd.DataFrame(columns=['Calculation view','Input calculation view'])\n",
    "table31 = pd.DataFrame(columns=['Calculation view','Data source','Columns used','Columns in source','Percentage columns used'])\n",
    "columns_tables = pd.read_excel(\"data/Columns_sources.xlsx\").dropna().reset_index(drop=True)\n",
    "for files in list_files:\n",
    "    calc_names.append(files.split('-')[1].split('.')[0])\n",
    "for i in info_calc.keys():\n",
    "    for j in range(len(info_calc[i])):\n",
    "        if info_calc[i]['LABEL_NODE'][j] in calc_names:\n",
    "            temp = {'Calculation view': i,'Input calculation view' : info_calc[i]['LABEL_NODE'][j]}\n",
    "            table22.loc[len(table22)] = temp\n",
    "        for k in range(len(columns_tables)):  \n",
    "            if info_calc[i]['LABEL_NODE'][j] == columns_tables['LABEL_NODE'][k] and info_calc[i]['FUNCTION'][j] == \"DataSources\":\n",
    "                temp = {'Calculation view': i,'Data source' : info_calc[i]['LABEL_NODE'][j], 'Columns used' : info_calc[i]['SOURCE_COUNT'][j], 'Columns in source' :  columns_tables['COUNT'][k], \"Percentage columns used\" : info_calc[i]['SOURCE_COUNT'][j]/columns_tables['COUNT'][k]}\n",
    "                table31.loc[len(table31)] = temp\n",
    "    \n",
    "table113 = trans_data.sort_values(by='Occurrences', ascending=False).head(5).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "table11.to_csv(f\"{save_DIR}/table11.csv\",index = False)\n",
    "table112.to_csv(f\"{save_DIR}/table112.csv\",index = False)\n",
    "table113.to_csv(f\"{save_DIR}/table113.csv\",index = False)\n",
    "table2122.to_csv(f\"{save_DIR}/table2122.csv\",index = False)\n",
    "table211.to_csv(f\"{save_DIR}/table211.csv\",index = False)\n",
    "table212.to_csv(f\"{save_DIR}/table212.csv\",index = False)\n",
    "table22.to_csv(f\"{save_DIR}/table22.csv\",index = False)\n",
    "table31.to_csv(f\"{save_DIR}/table31.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "isinstance() arg 2 must be a type, a tuple of types, or a union",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 142\u001b[0m\n\u001b[0;32m    136\u001b[0m         preprocessed_queries\u001b[38;5;241m.\u001b[39mappend(preprocessed_query)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preprocessed_queries\n\u001b[1;32m--> 142\u001b[0m preprocessed_queries \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/queries-txts/TEST.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 'data/queries-txts/WorldWideImporters 1.txt'\u001b[39;00m\n\u001b[0;32m    144\u001b[0m preprocessed_queries\n",
      "Cell \u001b[1;32mIn[31], line 130\u001b[0m, in \u001b[0;36mpreprocess_queries\u001b[1;34m(dir)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, query \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sql_queries):\n\u001b[0;32m    129\u001b[0m     ast \u001b[38;5;241m=\u001b[39m parse_one(query, read\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtsql\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m     subqueries \u001b[38;5;241m=\u001b[39m \u001b[43mextract_subqueries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mast\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     main_query \u001b[38;5;241m=\u001b[39m replace_subqueries_in_mainquery(ast, subqueries)\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m#main_query = replace_subqueries_in_mainquery(query, subqueries)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 44\u001b[0m, in \u001b[0;36mextract_subqueries\u001b[1;34m(ast)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(subquery_j) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(subquery_i) \u001b[38;5;129;01mand\u001b[39;00m j \u001b[38;5;241m!=\u001b[39m i:                      \n\u001b[0;32m     43\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43msubquery_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubquery_j\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;66;03m#.replace(sqlglot.exp.Table(this=\"subquery_x\"))\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m#subquery_i = subquery_i.replace(subquery_j, f\"subquery_{count}\")\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     nested_subqueries[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubquery_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m subquery_i\n",
      "File \u001b[1;32mc:\\Users\\PietroGarroni\\anaconda3\\envs\\sqlglot3\\Lib\\site-packages\\sqlglot\\expressions.py:354\u001b[0m, in \u001b[0;36mExpression.find\u001b[1;34m(self, bfs, *expression_types)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mexpression_types: t\u001b[38;5;241m.\u001b[39mType[E], bfs: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mOptional[E]:\n\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    Returns the first node in this tree which matches at least one of\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    the specified types.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        The node which matches the criteria or None if no such node was found.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;241m*\u001b[39mexpression_types, bfs\u001b[38;5;241m=\u001b[39mbfs), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\PietroGarroni\\anaconda3\\envs\\sqlglot3\\Lib\\site-packages\\sqlglot\\expressions.py:369\u001b[0m, in \u001b[0;36mExpression.find_all\u001b[1;34m(self, bfs, *expression_types)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03mReturns a generator object which visits all nodes in this tree and only\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03myields those that match at least one of the specified expression types.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    The generator object.\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expression, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwalk(bfs\u001b[38;5;241m=\u001b[39mbfs):\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expression, expression_types):\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m expression\n",
      "\u001b[1;31mTypeError\u001b[0m: isinstance() arg 2 must be a type, a tuple of types, or a union"
     ]
    }
   ],
   "source": [
    "from sqlglot import parse_one, exp\n",
    "from sqlglot.dialects.ma import MA\n",
    "from sqlglot.dialects.tsql import TSQL\n",
    "import sqlglot\n",
    "import copy\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def open_query(dir:str) -> list:\n",
    "    \"\"\"\n",
    "    Open sql queries from one text file\n",
    "    \"\"\" \n",
    "    with open(dir, 'r') as file: \n",
    "        file = file.read().strip().split(';')\n",
    "        sql_queries = [re.sub(r'\\s+', ' ', query.strip().replace('\\n', ' ').replace('\\t', ' ')) for query in file if query.strip()]\n",
    "    return sql_queries\n",
    "\n",
    "def transformer_functions(node):\n",
    "    \"\"\"\n",
    "    Replaces column objects within the functions with simple column names\n",
    "    \"\"\"\n",
    "    if isinstance(node, exp.Column):\n",
    "        return parse_one(node.name)\n",
    "    return node\n",
    "\n",
    "def extract_subqueries(ast: sqlglot.expressions) -> dict:\n",
    "    \"\"\"\n",
    "    Extract all subqueries from a query and saves them in a dictionary with structured format\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    selects = list(ast.find_all(exp.Select))  \n",
    "\n",
    "    selects = [select for select in selects] # problem with tsql conversion: column aliases get dropped in case of no transformation\n",
    "    nested_subqueries = {}\n",
    "\n",
    "    for i, subquery_i in enumerate(selects):\n",
    "        for j, subquery_j in enumerate(selects):\n",
    "    \n",
    "\n",
    "            if str(subquery_j) in str(subquery_i) and j != i:                      \n",
    "                count +=1\n",
    "                print(subquery_i.find(subquery_j))#.replace(sqlglot.exp.Table(this=\"subquery_x\"))\n",
    "\n",
    "                subquery_i = subquery_i.replace(subquery_j, f\"subquery_{count}\")\n",
    "                nested_subqueries[f\"subquery_{i}\"] = subquery_i\n",
    "                nested_subqueries[f\"subquery_{j}\"] = subquery_j\n",
    "\n",
    "            #elif i!=j and len(subquery_j) < len(subquery_i)-30:\n",
    "             #   pass\n",
    "                #print(subquery_j)\n",
    "                #print(subquery_i) \n",
    "                #print()\n",
    "                    \n",
    "\n",
    "    #main_query = nested_subqueries['subquery_0']\n",
    "\n",
    "    subqueries = {k: v for k, v in nested_subqueries.items() if k != 'subquery_0'}\n",
    "    print(count)\n",
    "\n",
    "    return subqueries\n",
    "\n",
    "\n",
    "def replace_nested_subqueries_in_subqueries(subqueries: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Replace nested subqueries in subqueries with the key\n",
    "    \"\"\"\n",
    "        \n",
    "    for i, query_i in subqueries.items():\n",
    "        for j, query_j in subqueries.items():\n",
    "      \n",
    "\n",
    "            if j in query_i:\n",
    "                subqueries[i] = subqueries[i].replace(j, query_j)\n",
    "    return subqueries\n",
    "\n",
    "\n",
    "def replace_subqueries_in_mainquery(ast: sqlglot.expressions, subqueries_global:dict) -> str:\n",
    "    \"\"\"\n",
    "    Replace nested subqueries in main_query with the key\n",
    "    \"\"\"\n",
    "    subqueries = subqueries_global.copy()\n",
    "    try:\n",
    "        main_query = list(repr((ast.find_all(exp.Create))))[0]\n",
    "    except:\n",
    "        print('error')\n",
    "        #main_query = str(list(ast.find_all(exp.Insert))[0])\n",
    "\n",
    "    subqueries = replace_nested_subqueries_in_subqueries(subqueries)\n",
    "\n",
    "    for key_i, value_i in subqueries.items():\n",
    "            \n",
    "        main_query = main_query.replace(f\"({str(value_i)})\", str(key_i))\n",
    "        main_query = main_query.replace(value_i, str(key_i))\n",
    "\n",
    "    return main_query\n",
    "\n",
    "\n",
    "#def replace_subqueries_in_mainquery(main_query: str, subqueries_global:dict) -> str:\n",
    "#    \"\"\"\n",
    "#    Replace nested subqueries in main_query with the key\n",
    "#    \"\"\"\n",
    "#    subqueries = subqueries_global.copy()\n",
    "#\n",
    "#    subqueries = replace_nested_subqueries_in_subqueries(subqueries)\n",
    "#\n",
    "#    for key_i, value_i in subqueries.items():\n",
    "#            \n",
    "#        main_query = main_query.replace(f\"({str(value_i)})\", str(key_i))\n",
    "#        main_query = main_query.replace(value_i, str(key_i))\n",
    "#\n",
    "#    return main_query\n",
    "\n",
    "\n",
    "def save_preprocessed_query(preprocessed_query, idx):\n",
    "    filename = f'data/preprocessed-queries/json_data{idx}.json'\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(preprocessed_query, json_file, indent=4)\n",
    "\n",
    "\n",
    "def preprocess_queries(dir:str) -> dict:\n",
    "    \"\"\"\n",
    "    Orchestrates the preprocessing and extraction of the SQL queries\n",
    "    \"\"\"\n",
    "    preprocessed_queries = []\n",
    "    sql_queries = open_query(dir)\n",
    "    for i, query in enumerate(sql_queries):\n",
    "        ast = parse_one(query, read=\"tsql\")\n",
    "        subqueries = extract_subqueries(ast)\n",
    "        main_query = replace_subqueries_in_mainquery(ast, subqueries)\n",
    "        #main_query = replace_subqueries_in_mainquery(query, subqueries)\n",
    "\n",
    "        preprocessed_query = {'modified_SQL_query': main_query, 'subquery_dictionary': subqueries}\n",
    "        save_preprocessed_query(preprocessed_query, i)\n",
    "        preprocessed_queries.append(preprocessed_query)\n",
    "\n",
    "    return preprocessed_queries\n",
    "\n",
    "\n",
    "\n",
    "preprocessed_queries = preprocess_queries('data/queries-txts/TEST.txt') # 'data/queries-txts/WorldWideImporters 1.txt'\n",
    "\n",
    "preprocessed_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlglot.expressions.Select"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlglot import parse_one, exp\n",
    "from sqlglot.dialects.ma import MA\n",
    "from sqlglot.dialects.tsql import TSQL\n",
    "import sqlglot\n",
    "import copy\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "query = \"\"\"CREATE VIEW INVESTOR_OVERVIEW AS\n",
    "SELECT\n",
    "    i.investor_id,\n",
    "    i.first_name,\n",
    "    i.last_name,\n",
    "    i.email,\n",
    "    p.portfolio_id,\n",
    "    p.portfolio_name,\n",
    "    recent_trades.trade_id,\n",
    "    recent_trades.trade_date,\n",
    "    recent_trades.ticker,\n",
    "    recent_trades.company_name,\n",
    "    recent_trades.trade_type,\n",
    "    recent_trades.quantity,\n",
    "    recent_trades.price_per_share,\n",
    "    recent_trades.total_trade_value,\n",
    "    CAST(dividends_received.total_dividends AS DECIMAL(10,2)) AS total_dividends,\n",
    "    CAST(average_performance.avg_performance AS DECIMAL(10,2)) AS average_performance,\n",
    "    CAST(total_investment.total_value AS DECIMAL(10,2)) AS total_investment_value\n",
    "FROM\n",
    "    investors i\n",
    "JOIN portfolios p ON i.investor_id = p.investor_id\n",
    "JOIN (\n",
    "    SELECT\n",
    "        t.trade_id,\n",
    "        t.portfolio_id,\n",
    "        s.ticker,\n",
    "        s.company_name,\n",
    "        t.trade_date,\n",
    "        t.trade_type,\n",
    "        t.quantity,\n",
    "        t.price_per_share,\n",
    "        (t.quantity * t.price_per_share) AS total_trade_value\n",
    "    FROM\n",
    "        trades t\n",
    "    JOIN stocks s ON t.stock_id = s.stock_id\n",
    "    WHERE\n",
    "        t.trade_date >= DATEADD(month, -1, GETDATE())\n",
    ") recent_trades ON p.portfolio_id = recent_trades.portfolio_id\n",
    "JOIN (\n",
    "    SELECT\n",
    "        t.portfolio_id,\n",
    "        SUM(d.dividend_amount * t.quantity) AS total_dividends\n",
    "    FROM\n",
    "        trades t\n",
    "    JOIN dividends d ON t.stock_id = d.stock_id\n",
    "    WHERE\n",
    "        d.dividend_date >= DATEADD(year, -1, GETDATE())\n",
    "    GROUP BY\n",
    "        t.portfolio_id\n",
    ") dividends_received ON p.portfolio_id = dividends_received.portfolio_id\n",
    "JOIN (\n",
    "    SELECT\n",
    "        t.portfolio_id,\n",
    "        AVG(md.closing_price) AS avg_performance\n",
    "    FROM\n",
    "        trades t\n",
    "    JOIN market_data md ON t.stock_id = md.stock_id\n",
    "    WHERE\n",
    "        md.market_date >= DATEADD(year, -1, GETDATE())\n",
    "    GROUP BY\n",
    "        t.portfolio_id\n",
    ") average_performance ON p.portfolio_id = average_performance.portfolio_id\n",
    "JOIN (\n",
    "    SELECT\n",
    "        t.portfolio_id,\n",
    "        SUM(t.quantity * md.closing_price) AS total_value\n",
    "    FROM\n",
    "        trades t\n",
    "    JOIN market_data md ON t.stock_id = md.stock_id\n",
    "    WHERE\n",
    "        md.market_date = (SELECT MAX(market_date) FROM market_data)\n",
    "    GROUP BY\n",
    "        t.portfolio_id\n",
    ") total_investment ON p.portfolio_id = total_investment.portfolio_id\n",
    "WHERE\n",
    "    i.join_date <= DATEADD(year, -1, GETDATE())\n",
    "ORDER BY\n",
    "    i.investor_id,\n",
    "    p.portfolio_id;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import sqlglot\n",
    "\n",
    "def extract_subqueries(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract all subqueries from a query and saves them in a dictionary with structured format\n",
    "    \"\"\"\n",
    "    ast = parse_one(query, dialect = 'tsql')\n",
    "    count = 0\n",
    "    selects = list(ast.find_all(exp.Select))   \n",
    "    selects = [select for select in selects] # problem with tsql conversion: column aliases get dropped in case of no transformation\n",
    "    subqueries = {}\n",
    "    \n",
    "    for i, select in enumerate(selects):\n",
    "        print(type(select))\n",
    "        subqueries[f'subquery_{i}'] = select\n",
    "\n",
    "\n",
    "    subqueries = {k: v for k, v in subqueries.items() if k != 'subquery_0'}\n",
    "\n",
    "\n",
    "    return subqueries\n",
    "\n",
    "\n",
    "subqueries = extract_subqueries(query)\n",
    "\n",
    "type(subqueries['subquery_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlglot.expressions.Select"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlglot\n",
    "\n",
    "def extract_subqueries(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract all subqueries from a query and saves them in a dictionary with structured format\n",
    "    \"\"\"\n",
    "    ast = parse_one(query, dialect = 'tsql')\n",
    "    count = 0\n",
    "    selects = list(ast.find_all(exp.Select))   \n",
    "    selects = [select for select in selects] # problem with tsql conversion: column aliases get dropped in case of no transformation\n",
    "    subqueries = {}\n",
    "    \n",
    "    for i, select in enumerate(selects):\n",
    "        print(type(select))\n",
    "        subqueries[f'subquery_{i}'] = select\n",
    "\n",
    "\n",
    "    subqueries = {k: v for k, v in subqueries.items() if k != 'subquery_0'}\n",
    "\n",
    "\n",
    "    return subqueries\n",
    "\n",
    "\n",
    "subqueries = extract_subqueries(query)\n",
    "\n",
    "type(subqueries['subquery_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query:\n",
      "CREATE VIEW INVESTOR_OVERVIEW AS\n",
      "SELECT\n",
      "    i.investor_id,\n",
      "    i.first_name,\n",
      "    i.last_name,\n",
      "    i.email,\n",
      "    p.portfolio_id,\n",
      "    p.portfolio_name,\n",
      "    recent_trades.trade_id,\n",
      "    recent_trades.trade_date,\n",
      "    recent_trades.ticker,\n",
      "    recent_trades.company_name,\n",
      "    recent_trades.trade_type,\n",
      "    recent_trades.quantity,\n",
      "    recent_trades.price_per_share,\n",
      "    recent_trades.total_trade_value,\n",
      "    CAST(dividends_received.total_dividends AS DECIMAL(10,2)) AS total_dividends,\n",
      "    CAST(average_performance.avg_performance AS DECIMAL(10,2)) AS average_performance,\n",
      "    CAST(total_investment.total_value AS DECIMAL(10,2)) AS total_investment_value\n",
      "FROM\n",
      "    investors i\n",
      "JOIN portfolios p ON i.investor_id = p.investor_id\n",
      "JOIN (\n",
      "    SELECT\n",
      "        t.trade_id,\n",
      "        t.portfolio_id,\n",
      "        s.ticker,\n",
      "        s.company_name,\n",
      "        t.trade_date,\n",
      "        t.trade_type,\n",
      "        t.quantity,\n",
      "        t.price_per_share,\n",
      "        (t.quantity * t.price_per_share) AS total_trade_value\n",
      "    FROM\n",
      "        trades t\n",
      "    JOIN stocks s ON t.stock_id = s.stock_id\n",
      "    WHERE\n",
      "        t.trade_date >= DATEADD(month, -1, GETDATE())\n",
      ") recent_trades ON p.portfolio_id = recent_trades.portfolio_id\n",
      "JOIN (\n",
      "    SELECT\n",
      "        t.portfolio_id,\n",
      "        SUM(d.dividend_amount * t.quantity) AS total_dividends\n",
      "    FROM\n",
      "        trades t\n",
      "    JOIN dividends d ON t.stock_id = d.stock_id\n",
      "    WHERE\n",
      "        d.dividend_date >= DATEADD(year, -1, GETDATE())\n",
      "    GROUP BY\n",
      "        t.portfolio_id\n",
      ") dividends_received ON p.portfolio_id = dividends_received.portfolio_id\n",
      "JOIN (\n",
      "    SELECT\n",
      "        t.portfolio_id,\n",
      "        AVG(md.closing_price) AS avg_performance\n",
      "    FROM\n",
      "        trades t\n",
      "    JOIN market_data md ON t.stock_id = md.stock_id\n",
      "    WHERE\n",
      "        md.market_date >= DATEADD(year, -1, GETDATE())\n",
      "    GROUP BY\n",
      "        t.portfolio_id\n",
      ") average_performance ON p.portfolio_id = average_performance.portfolio_id\n",
      "JOIN (\n",
      "    SELECT\n",
      "        t.portfolio_id,\n",
      "        SUM(t.quantity * md.closing_price) AS total_value\n",
      "    FROM\n",
      "        trades t\n",
      "    JOIN market_data md ON t.stock_id = md.stock_id\n",
      "    WHERE\n",
      "        md.market_date = (SELECT MAX(market_date) FROM market_data)\n",
      "    GROUP BY\n",
      "        t.portfolio_id\n",
      ") total_investment ON p.portfolio_id = total_investment.portfolio_id\n",
      "WHERE\n",
      "    i.join_date <= DATEADD(year, -1, GETDATE())\n",
      "ORDER BY\n",
      "    i.investor_id,\n",
      "    p.portfolio_id;\n",
      "\n",
      "\n",
      "Modified Query:\n",
      "CREATE VIEW INVESTOR_OVERVIEW AS SELECT i.investor_id, i.first_name, i.last_name, i.email, p.portfolio_id, p.portfolio_name, recent_trades.trade_id, recent_trades.trade_date, recent_trades.ticker, recent_trades.company_name, recent_trades.trade_type, recent_trades.quantity, recent_trades.price_per_share, recent_trades.total_trade_value, TYPE_CONVERSION(dividends_received.total_dividends) AS total_dividends, TYPE_CONVERSION(average_performance.avg_performance) AS average_performance, TYPE_CONVERSION(total_investment.total_value) AS total_investment_value FROM investors AS i JOIN portfolios AS p ON EQ(i.investor_id, p.investor_id) JOIN (subquery_1) AS recent_trades ON EQ(p.portfolio_id, recent_trades.portfolio_id) JOIN (subquery_2) AS dividends_received ON EQ(p.portfolio_id, dividends_received.portfolio_id) JOIN (subquery_3) AS average_performance ON EQ(p.portfolio_id, average_performance.portfolio_id) JOIN (subquery_4) AS total_investment ON EQ(p.portfolio_id, total_investment.portfolio_id) WHERE COMPARE(i.join_date, DATETIME_ADDITION('YEAR')) ORDER BY i.investor_id, p.portfolio_id\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT SUBQUERIES FROM MAIN QUERY WITHOUT STRINGS\n",
    "\n",
    "\n",
    "def replace_subquery_with_table(node):\n",
    "\n",
    "    if type(node) == sqlglot.expressions.Select:\n",
    "        for name, subquery in subqueries.items():    \n",
    "            if node.sql() == subquery.sql():  # Check if the node is a subquery\n",
    "                return sqlglot.exp.Table(this=name)  # Replace with a table node\n",
    "    return node\n",
    "\n",
    "\n",
    "# Parse the SQL query into an abstract syntax tree (AST)\n",
    "parsed = sqlglot.parse_one(query, dialect = 'tsql')\n",
    "\n",
    "# Apply the transformation to the AST\n",
    "transformed = parsed.transform(replace_subquery_with_table)\n",
    "\n",
    "# Generate the modified SQL query\n",
    "#new_query = transformed.sql()\n",
    "\n",
    "print(\"Original Query:\")\n",
    "print(query)\n",
    "print(\"\\nModified Query:\")\n",
    "print(transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 48\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m transformed\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, subquery \u001b[38;5;129;01min\u001b[39;00m \u001b[43msubqueries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Apply the transformation to the AST\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m replace_subquery_with_table_subquery(subquery)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# Generate the modified SQL query\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m#new_query = transformed.sql()\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# EXTRACT NESTED SUBQUERIES FROM SUBQUERIES WITHOUT STRINGS\n",
    "\n",
    "def extract_subqueries(ast: sqlglot.expressions.Select) -> dict:\n",
    "    \"\"\"\n",
    "    Extract all subqueries from a query and saves them in a dictionary with structured format\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    selects = list(ast.find_all(exp.Select))   \n",
    "    selects = [select for select in selects] # problem with tsql conversion: column aliases get dropped in case of no transformation\n",
    "    subqueries = {}\n",
    "    \n",
    "    for i, select in enumerate(selects):\n",
    "        print(type(select))\n",
    "        subqueries[f'subquery_{i}'] = select\n",
    "\n",
    "    subqueries = {k: v for k, v in subqueries.items() if k != 'subquery_0'}\n",
    "\n",
    "    return subqueries\n",
    "\n",
    "\n",
    "def replace_subquery_with_table(node):\n",
    "\n",
    "    if type(node) == sqlglot.expressions.Select:\n",
    "        for name, subquery, name_old, subquery_old in zip(subqueries.items(), old_subqueries.items()):    \n",
    "            if node.sql() == subquery.sql():  # Check if the node is a subquery\n",
    "                return sqlglot.exp.Table(this=name_old)  # Replace with a table node\n",
    "    return node\n",
    "\n",
    "def replace_subquery_with_table_subquery(node):\n",
    "\n",
    "    global old_subqueries\n",
    "    global subqueries \n",
    "\n",
    "\n",
    "    old_subqueries = subqueries.copy()\n",
    "    subqueries = extract_subqueries(node)\n",
    "    print(subqueries)\n",
    "\n",
    "\n",
    "    for select in node.find_all(exp.Select):\n",
    "        transformed = node.transform(replace_subquery_with_table)\n",
    "        return transformed\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "for name, subquery in subqueries.items():\n",
    "\n",
    "    # Apply the transformation to the AST\n",
    "    transformed = replace_subquery_with_table_subquery(subquery)\n",
    "\n",
    "    # Generate the modified SQL query\n",
    "    #new_query = transformed.sql()\n",
    "\n",
    "    print(\"Original Query:\")\n",
    "    print(subquery)\n",
    "    print(\"Modified Query:\")\n",
    "    print(transformed)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "<class 'sqlglot.expressions.Select'>\n",
      "SELECT * FROM (SELECT id, name FROM users) AS subquery\n",
      "*\n",
      "FROM (SELECT id, name FROM users) AS subquery\n",
      "(SELECT id, name FROM users) AS subquery\n",
      "SELECT id, name FROM users\n",
      "subquery\n",
      "subquery\n",
      "Original Query:\n",
      "\n",
      "SELECT *\n",
      "FROM (\n",
      "    SELECT id, name\n",
      "    FROM users\n",
      ") AS subquery\n",
      "\n",
      "\n",
      "Modified Query:\n",
      "SELECT * FROM (replaced_table) AS subquery\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rabo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
